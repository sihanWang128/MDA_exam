{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7208a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text \n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373a6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "additional= ['say','like','just','dont','don','im','it','ve','re','we',\n",
    "                'live','youll','youve','things','thing','youre','right','really','lot',\n",
    "                'make','know','people','way','day',\n",
    "                'little', 'maybe','men',\"americans\",\"america\",\n",
    "                'kind','heart',\"american\",'beverybrief','behooves','president','united',\"states\", \"s\", \"u\",\"evening\", \"afternoon\", \"welcome\",\n",
    "               \"everybody\", \"goodbye\", 'want', 'take', 'see', 'say', 'go','like','youre','ive','im','really','id','just','dont','didnt','thi','wa',\n",
    "                  'say','know','make','people',\"today\",'way','day','time','year','tonight', 'u', 'country', 'well']        \n",
    "stop_word_file=open('stopwords_file.txt', 'r')\n",
    "words=stop_word_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274497b5",
   "metadata": {},
   "source": [
    "From the data frame we created, the title and content still contain some strings which are not important.\n",
    "We create the cleanning functions for data pre-processing, removing all the non-important strings from all the speech content, and keep the nouns and verbs for further analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fcb8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.americanrhetoric.com/speeches/bara...</td>\n",
       "      <td>1 Thank you, thank you. My fellow citizens: I...</td>\n",
       "      <td>FIRST PRESIDENTIAL INAUGURAL ADDRESS</td>\n",
       "      <td>20 Jan 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.americanrhetoric.com/speeches/bara...</td>\n",
       "      <td>We begin this year and this  Administration i...</td>\n",
       "      <td>First Presidential Weekly Speech</td>\n",
       "      <td>24 Jan 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.americanrhetoric.com/speeches/bara...</td>\n",
       "      <td>Mr. Melhem: Mr. President, thank you for this...</td>\n",
       "      <td>Al-Arabiya Television Interview</td>\n",
       "      <td>26 Jan 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.americanrhetoric.com/speeches/bara...</td>\n",
       "      <td>Thank you, Tim, for your  hard work on this i...</td>\n",
       "      <td>Executive Compensation Salary Cap Speech</td>\n",
       "      <td>04 Feb 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.americanrhetoric.com/speeches/bara...</td>\n",
       "      <td>Good  evening, everybody. Please be seated. B...</td>\n",
       "      <td>First Prime Time Press Conference</td>\n",
       "      <td>09 Feb 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.americanrhetoric.com/speeches/bara...   \n",
       "1  https://www.americanrhetoric.com/speeches/bara...   \n",
       "2  https://www.americanrhetoric.com/speeches/bara...   \n",
       "3  https://www.americanrhetoric.com/speeches/bara...   \n",
       "4  https://www.americanrhetoric.com/speeches/bara...   \n",
       "\n",
       "                                             content  \\\n",
       "0   1 Thank you, thank you. My fellow citizens: I...   \n",
       "1   We begin this year and this  Administration i...   \n",
       "2   Mr. Melhem: Mr. President, thank you for this...   \n",
       "3   Thank you, Tim, for your  hard work on this i...   \n",
       "4   Good  evening, everybody. Please be seated. B...   \n",
       "\n",
       "                                      title         date  \n",
       "0      FIRST PRESIDENTIAL INAUGURAL ADDRESS  20 Jan 2009  \n",
       "1          First Presidential Weekly Speech  24 Jan 2009  \n",
       "2           Al-Arabiya Television Interview  26 Jan 2009  \n",
       "3  Executive Compensation Salary Cap Speech  04 Feb 2009  \n",
       "4         First Prime Time Press Conference  09 Feb 2009  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speech=pd.read_csv('obama_speech.csv').iloc[:,1:]\n",
    "df_speech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca290f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListToString(list):\n",
    "        string_words = ' '.join(list)\n",
    "        return string_words\n",
    "\n",
    "# string to list\n",
    "def StringToList(string):\n",
    "        listRes = list(string.split(\" \"))\n",
    "        return listRes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7e22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text, additional, words, stopwords):\n",
    "    \n",
    "        text=re.sub('--', ' ', text)\n",
    "        text=contractions.fix(text)  #change you're to you are\n",
    "        text=re.sub('\\x92', \"'\", text)\n",
    "        text=re.sub('\\s', ' ', text)\n",
    "        text=re.sub(\"[!@#$';.:]\", ' ',text)\n",
    "        #text= re.sub('\\((.*?\\))', '', text)\n",
    "        #text= re.sub('\\[.*?\\]', '', text)\n",
    "        #text= re.sub('[%s]' % re.escape(string.punctuation), '',text)\n",
    "        #text= re.sub('\\w*\\d\\w*', '', text)\n",
    "        text=re.sub(\"(\\s\\d+)\",\"\",text)  # remove numbers\n",
    "        text=re.sub('Book/CDs by Michael E. Eidenmuller, Published by \\r\\nMcGraw-Hill (2008)',\"\",text) \n",
    "        text=text.lower()\n",
    "        \n",
    "        ps = PorterStemmer()\n",
    "        text=ps.stem(text)\n",
    "        \n",
    "        # Remove the punctuations\n",
    "        tokens = word_tokenize(text)\n",
    "        # Lower the tokens\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # Remove stopword\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        # Lemmatize\n",
    "        tokens = [word for word in tokens if not word in stopwords]\n",
    "        tokens = [word for word in tokens if not word in additional]\n",
    "        tokens = [word for word in tokens if not word in words]\n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokens = [lemma.lemmatize(word, pos=\"v\") for word in tokens]\n",
    "        tokens = [lemma.lemmatize(word, pos=\"n\") for word in tokens]\n",
    "        \n",
    "        # list to string\n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bccbc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_extract(text):\n",
    "        nouns = []  # empty to array to hold all nouns\n",
    "        text = StringToList(text)\n",
    "        for word, pos in nltk.pos_tag(text):\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(word)\n",
    "                string_nouns = ListToString(nouns)\n",
    "        return string_nouns\n",
    "\n",
    "def verbs_extract(text):\n",
    "    verbs=[]\n",
    "    text=StringToList(text)\n",
    "    string_verbs=''\n",
    "    for word, pos in nltk.pos_tag(text):\n",
    "        if (pos=='VB' or pos=='VBP' or pos=='VBZ'):\n",
    "            verbs.append(word)\n",
    "            string_verbs=ListToString(verbs)\n",
    "    return string_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85b2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # remove useless strings in content and title\n",
    "df_speech['clean_content']=[clean_text(str(i), additional, words, stopwords) for i in df_speech[\"content\"].values.tolist()]\n",
    "df_speech['content_nouns']=[nouns_extract(i) for i in df_speech['clean_content'].values.tolist()]\n",
    "df_speech['content_verbs']=[verbs_extract(i) for i in df_speech['clean_content'].values.tolist()]\n",
    "    \n",
    "    \n",
    "    # create obama_clean.csv for topic modelling and sentimental analysis\n",
    "df_speech.to_csv('obama_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac8862",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
